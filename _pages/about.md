---
permalink: /
title: ""
excerpt: "About me"
author_profile: true
redirect_from:

- /about/
- /about.html

---
I am looking for PhD positions for 24 fall.  

I am currently a Master's student in Computer Science at UMass Amherst where I am lucky to be advised by Professor [Andrew McCallum](https://people.cs.umass.edu/~mccallum/) and Professor [Hong Yu](https://www.cics.umass.edu/faculty/directory/hong_yu). I finished my undergraduate study in Computer Science at HKUST in 2022. In the past, I was a research assistant at [SMART lab](https://hkustsmartlab.netlify.app/) working on medical image analysis with Professor [Hao Chen](https://cse.hkust.edu.hk/~jhc/) and an NLP research intern at [IDEA](https://www.idea.edu.cn/en/about-team.html).  

**Research Interests**. I am especially interested in label-efficient learning / weakly-supervised learning and improving the robustness and interpretability of neural networks. I am particularly enthusiastic about applying the implication to biology and medicine where labeled data are scarce and trustworthiness is valued. Correspondingly, I have worked on various tasks with diverse kinds of data involving texts, images and videos. I am also broadly interested in explainability,  multimodality and neural science.  Currently, I focus on studying large language models, including understanding in-context learning (TEACH @ ICML23), knowledge distillation from LLMs (under review), applying LLMs to clinical QA (in preparation) and debiasing LLMs (in preparation). 


Apart from research, I am also interested in entrepreneurship which helps me consider my future research area. I took a minor in entrepreneurship at HKUST. My internship at the CTO lab of [IDEA](https://www.idea.edu.cn/en) was also about collaborating with a unicorn startup. In my spare time, I love photography and architecture. Check my instagram ([*zzzh_nick*](https://instagram.com/zzzh_nick?igshid=YmMyMTA2M2Y=)).  

## Publications  
### 2023  
- **Mist-KD: Multi-stage Knowledge Distillation for Parsing**\
Jiachen Zhao\*, Andrew Drozdov\*, Wenlong Zhao, Benjamin Rozonoyer, Yamini Kashyap, Mehek Tulsyan, Jay-Yoon Lee, Mohit Iyyer, Andrew McCallum\
Under review.

- **Student as an Inherent Denoiser of Noisy Teacher**\
Jiachen Zhao\
Under review.  
  
- **In-Context Exemplars as Clues to Retrieving from Large Associative Memory**\
Jiachen Zhao\
Neural Conversational AI Workshop @ ICML-23.  
[TL;DR] *We reinterpret In-Context Learning (ICL) of LLMs as contextual retrieval from associative memory motivated by the fact that no gradients update occurs in ICL and
analyze error bounds of retrieval errors. We relate ICL to humans' memory system and show some biological plausibility. We also discuss the implication of our theory to exemplar selection and chain-of-thought.*

### 2022 *(my undergraduate work)*
- **Adaptive Fusion of Deep Learning with Statistical Anatomical Knowledge for Robust Patella Segmentation from CT Images**\
Jiachen Zhao, Tianshu Jiang, Yi Lin, Justin Chan, Ping-Keung Lewis Chan, Chunyi Wen, Hao Chen\
Under review.  
[TL;DR] *Neural networks (NNs) for image segmentation are vulnerable to corrupted images and require sufficient labeled training data. To handle such issues, we propose to combine neural networks with statistical shape models (SSMs) and our method can automatically determine the contribution of NNs and SSMs during combination by leveraging proposed statistical methods.*

- **Trigger-free Event Detection via Derangement Reading Comprehension**\
Jiachen Zhao, Haiqin Yang\
*arXiv*, 2022. [link](https://arxiv.org/pdf/2208.09659.pdf)  
[TL;DR] *This work tries to improve the performance of event detection when human-labeled ``triggers'' are not available (the word in a sentence that is the most representative of the sentence and very costly to annotate).  We reformulate this event classification problem into a multiple-choice QA task to better leverage self-attention. We also propose a token derangement module to tackle the imbalanced learning issue.  Our trigger-free method can reach SOTA performance and even outperform approaches relying on triggers.*






